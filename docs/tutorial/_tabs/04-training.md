---
layout: post
icon: fas fa-play-circle
title: Training
date: 2024-10-02
toc: true
---


### [**Training** â€” Get the Ball Rolling](../04-training)
Training on this scale also presents unique challenges; here's what we'll cover.
- **Training Configurations**: We'll suggest settings for a speed run (18k GPU hours) and a full training run (100k GPU hours) for you to choose from and discuss what to expect from each. We'll also share intermediate and final results for our runs and discuss the two setups that we've tested.
- **Starting and Monitoring Training on a Cluster**: Open-Sora is built on top of the [ColossalAI launcher](https://colossalai.org/). We'll start by simply providing the commands to get you started and how to monitor loss curves in [weights and biases](https://wandb.com).
- **Evaluating Model Quality**: Understand how to evaluate model performance using a separate inference server.
- **Monitoring Cluster Health**: ?
- **Optimizing Performance**: We'll discuss how to identify and optimize bottlenecks in a multi-node training setup to increase training speed beyond just using faster models.

